{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read access confirmed. Sample document: {'_id': ObjectId('648437b0f0f526c7794ff3eb'), 'item_id': 4, 'colors': 'Green', 'discount': 0, 'discount_codes': ['Duke10', 'tysm10'], 'image_urls': ['https://images.urbndata.com/is/image/UrbanOutfitters/81080202_001_b?$xlarge$&fit=constrain&qlt=80&wid=640', 'https://images.urbndata.com/is/image/UrbanOutfitters/81080202_001_d?$xlarge$&fit=constrain&qlt=80&wid=640', 'https://images.urbndata.com/is/image/UrbanOutfitters/81080202_001_e?$xlarge$&fit=constrain&qlt=80&wid=640', 'https://images.urbndata.com/is/image/UrbanOutfitters/81080202_001_f?$xlarge$&fit=constrain&qlt=80&wid=640', 'https://images.urbndata.com/is/image/UrbanOutfitters/81080202_001_g?$xlarge$&fit=constrain&qlt=80&wid=640'], 'item_type': 'Midi Dress', 'item_url': 'https://www.urbanoutfitters.com/shop/uo-azelia-convertible-tube-dress?category=dresses&color=001&type=REGULAR', 'linked_items': [], 'linked_items_colors': [], 'long_description': 'WE NO LONGER SELL URBAN OUTFITTERS UO tube dress that offers two looks in one: wear it as a strapless midi dress or midi skirt with a bodycon fit throughout. Find it only at Urban Outfitters.', 'old_price': 0, 'price': 49, 'product_brand': 'Urban Outfitters', 'short_description': 'Azelia Convertible Tube Dress', 'sizes': {}, 'tags': [], 'user_reviews': [], 'available_sizes': [], 'is_sale': 'false', 'deleted': 'true', 'actual_color': 'Green'}\n",
      "CLIPModel: CLIPModel(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (position_embedding): Embedding(77, 512)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "      (position_embedding): Embedding(50, 768)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
      ")\n",
      "CLIPProcessor: CLIPProcessor:\n",
      "- image_processor: CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: CLIPTokenizerFast(name_or_path='patrickjohncyh/fashion-clip', vocab_size=49408, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"CLIPProcessor\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "# connect to Mongo\n",
    "client = MongoClient(\"mongodb+srv://rahul:Dhruvie12345@itemset.zl2kjim.mongodb.net/?retryWrites=true&w=majority&appName=ItemSet\")\n",
    "db = client[\"development\"]\n",
    "collection = db[\"products\"]\n",
    "\n",
    "# Retrieve one item from the collection\n",
    "try:\n",
    "    document = collection.find_one()\n",
    "    if document:\n",
    "        print(\"Read access confirmed. Sample document:\", document)\n",
    "    else:\n",
    "        print(\"Connected but no documents found in the collection.\")\n",
    "    model = CLIPModel.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
    "    print(\"CLIPModel:\", model)\n",
    "    print(\"CLIPProcessor:\", processor)\n",
    "except Exception as e:\n",
    "    print(\"Read access failed:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 random items have been saved to data/items.json\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "def clean_description(text):\n",
    "    # Remove escape sequences\n",
    "    text = text.encode('utf-8').decode('unicode_escape')\n",
    "    # Remove special characters and normalize whitespace\n",
    "    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "client = MongoClient(\"mongodb+srv://rahul:Dhruvie12345@itemset.zl2kjim.mongodb.net/?retryWrites=true&w=majority&appName=ItemSet\")\n",
    "db = client[\"development\"]\n",
    "collection = db[\"products\"]\n",
    "\n",
    "# Fetch 100 random items\n",
    "items = list(collection.aggregate([\n",
    "    { \"$sample\": { \"size\": 100 } }\n",
    "]))\n",
    "\n",
    "# Format the items\n",
    "formatted_items = {}\n",
    "for index, item in enumerate(items):\n",
    "    formatted_item = {\n",
    "        \"image_urls\": item.get(\"image_urls\", []),\n",
    "        \"product_name\": item.get(\"short_description\", \"default\"),\n",
    "        \"brand\": item.get(\"product_brand\", \"default\"),\n",
    "        \"long_description\": clean_description(item.get(\"long_description\", \"default\")),\n",
    "        \"color\": item.get(\"colors\", \"default\"),\n",
    "        \"item_type\": item.get(\"item_type\", \"default\"),\n",
    "        \"price\": item.get(\"price\", 0)\n",
    "    }\n",
    "    formatted_items[str(index)] = formatted_item\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"data/items.json\", \"w\") as f:\n",
    "    json.dump(formatted_items, f, indent=4)\n",
    "\n",
    "print(\"100 random items have been saved to data/items.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install transformers Pillow requests numpy nomic\n",
    "%pip install --upgrade nomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (520661867.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    nomic login\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 28.50% (285/1000)Error processing image for item 286\n",
      "Progress: 28.60% (286/1000)Error processing image for item 287\n",
      "Progress: 28.70% (287/1000)Error processing image for item 288\n",
      "Progress: 28.80% (288/1000)Error processing image for item 289\n",
      "Progress: 28.90% (289/1000)Error processing image for item 290\n",
      "Progress: 29.00% (290/1000)Error processing image for item 291\n",
      "Progress: 29.10% (291/1000)Error processing image for item 292\n",
      "Progress: 29.20% (292/1000)Error processing image for item 293\n",
      "Progress: 29.30% (293/1000)Error processing image for item 294\n",
      "Progress: 29.40% (294/1000)Error processing image for item 295\n",
      "Progress: 29.50% (295/1000)Error processing image for item 296\n",
      "Progress: 29.60% (296/1000)Error processing image for item 297\n",
      "Progress: 29.70% (297/1000)Error processing image for item 298\n",
      "Progress: 29.80% (298/1000)Error processing image for item 299\n",
      "Progress: 29.90% (299/1000)Error processing image for item 300\n",
      "Progress: 30.00% (300/1000)Error processing image for item 301\n",
      "Progress: 30.10% (301/1000)Error processing image for item 302\n",
      "Progress: 30.20% (302/1000)Error processing image for item 303\n",
      "Progress: 30.30% (303/1000)Error processing image for item 304\n",
      "Progress: 30.40% (304/1000)Error processing image for item 305\n",
      "Progress: 30.50% (305/1000)Error processing image for item 306\n",
      "Progress: 30.60% (306/1000)Error processing image for item 307\n",
      "Progress: 30.70% (307/1000)Error processing image for item 308\n",
      "Progress: 30.80% (308/1000)Error processing image for item 309\n",
      "Progress: 30.90% (309/1000)Error processing image for item 310\n",
      "Progress: 31.00% (310/1000)Error processing image for item 311\n",
      "Progress: 31.10% (311/1000)Error processing image for item 312\n",
      "Progress: 31.20% (312/1000)Error processing image for item 313\n",
      "Progress: 31.30% (313/1000)Error processing image for item 314\n",
      "Progress: 31.40% (314/1000)Error processing image for item 315\n",
      "Progress: 31.50% (315/1000)Error processing image for item 316\n",
      "Progress: 31.60% (316/1000)Error processing image for item 317\n",
      "Progress: 31.70% (317/1000)Error processing image for item 318\n",
      "Progress: 31.80% (318/1000)Error processing image for item 319\n",
      "Progress: 31.90% (319/1000)Error processing image for item 320\n",
      "Progress: 32.00% (320/1000)Error processing image for item 321\n",
      "Progress: 32.10% (321/1000)Error processing image for item 322\n",
      "Progress: 32.20% (322/1000)Error processing image for item 323\n",
      "Progress: 32.30% (323/1000)Error processing image for item 324\n",
      "Progress: 32.40% (324/1000)Error processing image for item 325\n",
      "Progress: 32.50% (325/1000)Error processing image for item 326\n",
      "Progress: 32.60% (326/1000)Error processing image for item 327\n",
      "Progress: 32.70% (327/1000)Error processing image for item 328\n",
      "Progress: 32.80% (328/1000)Error processing image for item 329\n",
      "Progress: 32.90% (329/1000)Error processing image for item 330\n",
      "Progress: 45.00% (450/1000)Error processing image for item 451\n",
      "Progress: 45.10% (451/1000)Error processing image for item 452\n",
      "Progress: 45.20% (452/1000)Error processing image for item 453\n",
      "Progress: 45.30% (453/1000)Error processing image for item 454\n",
      "Progress: 45.40% (454/1000)Error processing image for item 455\n",
      "Progress: 45.50% (455/1000)Error processing image for item 456\n",
      "Progress: 45.60% (456/1000)Error processing image for item 457\n",
      "Progress: 45.70% (457/1000)Error processing image for item 458\n",
      "Progress: 45.80% (458/1000)Error processing image for item 459\n",
      "Progress: 45.90% (459/1000)Error processing image for item 460\n",
      "Progress: 46.00% (460/1000)Error processing image for item 461\n",
      "Progress: 46.10% (461/1000)Error processing image for item 462\n",
      "Progress: 46.20% (462/1000)Error processing image for item 463\n",
      "Progress: 46.30% (463/1000)Error processing image for item 464\n",
      "Progress: 46.40% (464/1000)Error processing image for item 465\n",
      "Progress: 46.50% (465/1000)Error processing image for item 466\n",
      "Progress: 46.60% (466/1000)Error processing image for item 467\n",
      "Progress: 46.70% (467/1000)Error processing image for item 468\n",
      "Progress: 46.80% (468/1000)Error processing image for item 469\n",
      "Progress: 46.90% (469/1000)Error processing image for item 470\n",
      "Progress: 47.00% (470/1000)Error processing image for item 471\n",
      "Progress: 47.10% (471/1000)Error processing image for item 472\n",
      "Progress: 47.20% (472/1000)Error processing image for item 473\n",
      "Progress: 47.30% (473/1000)Error processing image for item 474\n",
      "Progress: 47.40% (474/1000)Error processing image for item 475\n",
      "Progress: 47.50% (475/1000)Error processing image for item 476\n",
      "Progress: 47.60% (476/1000)Error processing image for item 477\n",
      "Progress: 47.70% (477/1000)Error processing image for item 478\n",
      "Progress: 47.80% (478/1000)Error processing image for item 479\n",
      "Progress: 47.90% (479/1000)Error processing image for item 480\n",
      "Progress: 48.00% (480/1000)Error processing image for item 481\n",
      "Progress: 48.10% (481/1000)Error processing image for item 482\n",
      "Progress: 48.20% (482/1000)Error processing image for item 483\n",
      "Progress: 48.30% (483/1000)Error processing image for item 484\n",
      "Progress: 48.40% (484/1000)Error processing image for item 485\n",
      "Progress: 48.50% (485/1000)Error processing image for item 486\n",
      "Progress: 48.60% (486/1000)Error processing image for item 487\n",
      "Progress: 48.70% (487/1000)Error processing image for item 488\n",
      "Progress: 48.80% (488/1000)Error processing image for item 489\n",
      "Progress: 60.50% (605/1000)Error processing image for item 606\n",
      "Progress: 61.20% (612/1000)Error processing image for item 613\n",
      "Progress: 61.50% (615/1000)Error processing image for item 616\n",
      "Progress: 62.10% (621/1000)Error processing image for item 622\n",
      "Progress: 62.80% (628/1000)Error processing image for item 629\n",
      "Progress: 64.30% (643/1000)Error processing image for item 644\n",
      "Progress: 65.30% (653/1000)Error processing image for item 654\n",
      "Progress: 65.50% (655/1000)Error processing image for item 656\n",
      "Progress: 67.40% (674/1000)Error processing image for item 675\n",
      "Progress: 67.50% (675/1000)Error processing image for item 676\n",
      "Progress: 67.60% (676/1000)Error processing image for item 677\n",
      "Progress: 68.20% (682/1000)Error processing image for item 683\n",
      "Progress: 68.40% (684/1000)Error processing image for item 685\n",
      "Progress: 68.70% (687/1000)Error processing image for item 688\n",
      "Progress: 69.10% (691/1000)Error processing image for item 692\n",
      "Progress: 69.40% (694/1000)Error processing image for item 695\n",
      "Progress: 69.80% (698/1000)Error processing image for item 699\n",
      "Progress: 70.70% (707/1000)Error processing image for item 708\n",
      "Progress: 71.00% (710/1000)Error processing image for item 711\n",
      "Progress: 71.80% (718/1000)Error processing image for item 719\n",
      "Progress: 85.00% (850/1000)Error processing image for item 851\n",
      "Progress: 85.20% (852/1000)Error processing image for item 853\n",
      "Progress: 85.90% (859/1000)Error processing image for item 860\n",
      "Progress: 86.00% (860/1000)Error processing image for item 861\n",
      "Progress: 86.20% (862/1000)Error processing image for item 863\n",
      "Progress: 86.30% (863/1000)Error processing image for item 864\n",
      "Progress: 86.60% (866/1000)Error processing image for item 867\n",
      "Progress: 86.70% (867/1000)Error processing image for item 868\n",
      "Progress: 87.00% (870/1000)Error processing image for item 871\n",
      "Progress: 87.30% (873/1000)Error processing image for item 874\n",
      "Progress: 87.40% (874/1000)Error processing image for item 875\n",
      "Progress: 87.60% (876/1000)Error processing image for item 877\n",
      "Progress: 87.70% (877/1000)Error processing image for item 878\n",
      "Progress: 87.90% (879/1000)Error processing image for item 880\n",
      "Progress: 88.00% (880/1000)Error processing image for item 881\n",
      "Progress: 88.10% (881/1000)Error processing image for item 882\n",
      "Progress: 88.30% (883/1000)Error processing image for item 884\n",
      "Progress: 89.40% (894/1000)Error processing image for item 895\n",
      "Progress: 89.50% (895/1000)Error processing image for item 896\n",
      "Progress: 89.90% (899/1000)Error processing image for item 900\n",
      "Progress: 90.00% (900/1000)Error processing image for item 901\n",
      "Progress: 90.30% (903/1000)Error processing image for item 904\n",
      "Progress: 91.20% (912/1000)Error processing image for item 913\n",
      "Progress: 91.40% (914/1000)Error processing image for item 915\n",
      "Progress: 91.80% (918/1000)Error processing image for item 919\n",
      "Progress: 92.30% (923/1000)Error processing image for item 924\n",
      "Progress: 92.40% (924/1000)Error processing image for item 925\n",
      "Progress: 92.50% (925/1000)Error processing image for item 926\n",
      "Progress: 93.40% (934/1000)Error processing image for item 935\n",
      "Progress: 93.70% (937/1000)Error processing image for item 938\n",
      "Progress: 94.80% (948/1000)Error processing image for item 949\n",
      "Progress: 100.00% (1000/1000)\n",
      "Embeddings created successfully!\n",
      "Embeddings saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the items data\n",
    "# data_source = 'data/items.json'\n",
    "data_source = 'data/1000_items.json'\n",
    "with open(data_source, 'r') as f:\n",
    "    items = json.load(f)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Text embedding function\n",
    "def get_text_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Image embedding function\n",
    "def get_image_embedding(image_url, model, transform):\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    img_t = transform(img)\n",
    "    batch_t = torch.unsqueeze(img_t, 0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model(batch_t)\n",
    "    return embedding.squeeze().cpu().numpy()\n",
    "\n",
    "# Load pre-trained models\n",
    "text_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "text_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "image_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1).to(device)\n",
    "image_model = torch.nn.Sequential(*(list(image_model.children())[:-1]))\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Generate embeddings for each item\n",
    "embeddings = {}\n",
    "total_items = len(items)\n",
    "for index, (item_id, item_data) in enumerate(items.items(), 1):\n",
    "    # Text embedding\n",
    "    text = f\"{item_data['product_name']} {item_data['long_description']} {item_data['color']} {item_data['price']}\"\n",
    "    text_emb = get_text_embedding(text, text_model, text_tokenizer)\n",
    "    \n",
    "    # Image embedding\n",
    "    image_url = item_data['image_urls'][0]\n",
    "    try:\n",
    "        image_emb = get_image_embedding(image_url, image_model, image_transform)\n",
    "    except:\n",
    "        print(f\"Error processing image for item {item_id}\")\n",
    "        image_emb = np.zeros(2048)  # ResNet50 output size\n",
    "    \n",
    "    # Combine embeddings\n",
    "    combined_emb = np.concatenate([text_emb, image_emb])\n",
    "    embeddings[item_id] = combined_emb\n",
    "    \n",
    "    # Print progress\n",
    "    progress = (index / total_items) * 100\n",
    "    print(f\"\\rProgress: {progress:.2f}% ({index}/{total_items})\", end=\"\", flush=True)\n",
    "\n",
    "print(\"\\nEmbeddings created successfully!\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save('data/item_embeddings.npy', embeddings)\n",
    "\n",
    "print(\"Embeddings saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-11 17:46:58.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_create_project\u001b[0m:\u001b[36m857\u001b[0m - \u001b[1mCreating dataset `mens-embeddings`\u001b[0m\n",
      "\u001b[32m2024-08-11 17:46:58.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_data\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mUploading data to Atlas.\u001b[0m\n",
      "1it [00:00,  1.13it/s]\n",
      "\u001b[32m2024-08-11 17:46:59.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_add_data\u001b[0m:\u001b[36m1666\u001b[0m - \u001b[1mUpload succeeded.\u001b[0m\n",
      "\u001b[32m2024-08-11 17:46:59.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_data\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1m`rahul-styl-men/mens-embeddings`: Data upload succeeded to dataset`\u001b[0m\n",
      "\u001b[32m2024-08-11 17:46:59.521\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36mcreate_index\u001b[0m:\u001b[36m1111\u001b[0m - \u001b[33m\u001b[1mYou did not specify the `topic_label_field` option in your topic_model, your dataset will not contain auto-labeled topics.\u001b[0m\n",
      "\u001b[32m2024-08-11 17:47:00.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36mcreate_index\u001b[0m:\u001b[36m1262\u001b[0m - \u001b[1mCreated map `mens_embeddings` in dataset `rahul-styl-men/mens-embeddings`: https://atlas.nomic.ai/data/rahul-styl-men/mens-embeddings\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AtlasDataset' object has no attribute 'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create and upload project to Nomic Atlas\u001b[39;00m\n\u001b[1;32m     26\u001b[0m project \u001b[38;5;241m=\u001b[39m atlas\u001b[38;5;241m.\u001b[39mmap_data(\n\u001b[1;32m     27\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     28\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings_array,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     is_public\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AtlasDataset' object has no attribute 'url'"
     ]
    }
   ],
   "source": [
    "from nomic import atlas\n",
    "import numpy as np\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load('data/item_embeddings.npy', allow_pickle=True).item()\n",
    "\n",
    "# Prepare data for Nomic Atlas\n",
    "data = []\n",
    "embeddings_list = []\n",
    "for item_id, embedding in embeddings.items():\n",
    "    item_data = items[item_id]\n",
    "    data.append({\n",
    "        'id': item_id,\n",
    "        'product_name': item_data['product_name'],\n",
    "        'brand': item_data['brand'],\n",
    "        'long_description': item_data['long_description'],\n",
    "        'color': item_data['color'],\n",
    "        'item_type': item_data['item_type'],\n",
    "        'price': item_data['price'],\n",
    "        'image_url': item_data['image_urls'][0]\n",
    "    })\n",
    "    embeddings_list.append(embedding)\n",
    "embeddings_array = np.array(embeddings_list)\n",
    "\n",
    "# Create and upload project to Nomic Atlas\n",
    "project = atlas.map_data(\n",
    "    data=data,\n",
    "    embeddings=embeddings_array,\n",
    "    id_field='id',\n",
    "    identifier='mens_embeddings',\n",
    "    description=\"Combined text and image embeddings for men's clothing items\",\n",
    "    is_public=True\n",
    ")\n",
    "\n",
    "print(f\"Project created: {project.url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
